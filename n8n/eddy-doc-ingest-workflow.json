{
  "name": "EDDY Doc Ingest",
  "nodes": [
    {
      "parameters": {
        "rule": {
          "interval": [
            {
              "field": "minutes",
              "minutesInterval": 15
            }
          ]
        }
      },
      "id": "eddy-ingest-schedule",
      "name": "â° Scan Schedule (15min)",
      "type": "n8n-nodes-base.scheduleTrigger",
      "typeVersion": 1.2,
      "position": [100, 400],
      "webhookId": null
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "eddy/doc-ingest",
        "responseMode": "responseNode",
        "options": {
          "allowedOrigins": "*"
        }
      },
      "id": "eddy-ingest-webhook",
      "name": "ğŸŒ Webhook Ingest",
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2,
      "position": [100, 700],
      "webhookId": "eddy-doc-ingest-00000010"
    },
    {
      "parameters": {
        "jsCode": "// Scan /eddy-inbox/pending/ fÃ¼r neue Dokumente\nconst fs = require('fs');\nconst path = require('path');\nconst crypto = require('crypto');\n\nconst INBOX_DIR = '/eddy-inbox/pending';\nconst SUPPORTED_TYPES = ['.pdf', '.txt', '.md', '.docx'];\n\ntry {\n  if (!fs.existsSync(INBOX_DIR)) {\n    return [{ json: { error: 'Inbox directory not found', path: INBOX_DIR, files: [] } }];\n  }\n\n  const files = fs.readdirSync(INBOX_DIR)\n    .filter(f => {\n      const ext = path.extname(f).toLowerCase();\n      return SUPPORTED_TYPES.includes(ext) && !f.startsWith('.');\n    })\n    .map(f => {\n      const filePath = path.join(INBOX_DIR, f);\n      const stats = fs.statSync(filePath);\n      const fileBuffer = fs.readFileSync(filePath);\n      const fileHash = crypto.createHash('sha256').update(fileBuffer).digest('hex');\n      const ext = path.extname(f).toLowerCase().replace('.', '');\n      \n      return {\n        json: {\n          fileName: f,\n          filePath: filePath,\n          sourceType: ext,\n          fileHash: fileHash,\n          fileSize: stats.size,\n          modifiedAt: stats.mtime.toISOString()\n        }\n      };\n    });\n\n  if (files.length === 0) {\n    return [{ json: { status: 'empty', message: 'No new files in inbox' } }];\n  }\n\n  return files;\n} catch (err) {\n  return [{ json: { error: err.message } }];\n}"
      },
      "id": "eddy-ingest-scan",
      "name": "ğŸ“‚ Scan Inbox",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [340, 400]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "combinator": "and",
          "conditions": [
            {
              "leftValue": "={{ $json.fileName }}",
              "rightValue": "",
              "operator": {
                "type": "string",
                "operation": "isNotEmpty"
              }
            }
          ]
        }
      },
      "id": "eddy-ingest-has-files",
      "name": "ğŸ“‹ Has Files?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [560, 400]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT * FROM check_file_ingested($1);",
        "options": {
          "queryReplacement": "={{ [$json.fileHash] }}"
        }
      },
      "id": "eddy-ingest-dedup",
      "name": "ğŸ” Duplikat-Check",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [780, 400],
      "credentials": {
        "postgres": {
          "id": "Jq2IeHXVMOnpk0fI",
          "name": "eddy-knowledge-postgres"
        }
      }
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "combinator": "and",
          "conditions": [
            {
              "leftValue": "={{ $json.is_ingested }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "notTrue"
              }
            }
          ]
        }
      },
      "id": "eddy-ingest-is-new",
      "name": "ğŸ†• Noch nicht ingestiert?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [1000, 400]
    },
    {
      "parameters": {
        "jsCode": "// Text-Extraktion basierend auf Dateityp\nconst fs = require('fs');\n\nconst filePath = $('ğŸ“‹ Has Files?').first().json.filePath;\nconst sourceType = $('ğŸ“‹ Has Files?').first().json.sourceType;\nconst fileName = $('ğŸ“‹ Has Files?').first().json.fileName;\nconst fileHash = $('ğŸ“‹ Has Files?').first().json.fileHash;\nconst modifiedAt = $('ğŸ“‹ Has Files?').first().json.modifiedAt;\n\nlet content = '';\n\ntry {\n  if (sourceType === 'txt' || sourceType === 'md') {\n    // Plaintext/Markdown: direkt lesen\n    content = fs.readFileSync(filePath, 'utf-8');\n  } else if (sourceType === 'pdf') {\n    // PDF: Versuche text-basierte Extraktion\n    const { execSync } = require('child_process');\n    try {\n      content = execSync(`pdftotext -layout \"${filePath}\" -`, { encoding: 'utf-8', timeout: 30000 });\n    } catch (e) {\n      // Fallback: Binary als Text lesen (fÃ¼r simple PDFs)\n      content = fs.readFileSync(filePath, 'utf-8').replace(/[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]/g, ' ');\n    }\n  } else if (sourceType === 'docx') {\n    const { execSync } = require('child_process');\n    try {\n      content = execSync(`pandoc -t plain \"${filePath}\"`, { encoding: 'utf-8', timeout: 30000 });\n    } catch (e) {\n      content = '[DOCX extraction failed - install pandoc in n8n container]';\n    }\n  }\n\n  if (!content || content.trim().length < 10) {\n    return [{ json: { error: 'No content extracted', fileName, sourceType } }];\n  }\n\n  return [{\n    json: {\n      content: content.trim(),\n      fileName,\n      filePath,\n      sourceType,\n      fileHash,\n      modifiedAt,\n      contentLength: content.trim().length\n    }\n  }];\n} catch (err) {\n  return [{ json: { error: err.message, fileName, sourceType } }];\n}"
      },
      "id": "eddy-ingest-extract",
      "name": "ğŸ“„ Text Extraction",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1220, 400]
    },
    {
      "parameters": {
        "jsCode": "// Chunking mit Overlap\nconst CHUNK_SIZE = 800;     // ~500 Token\nconst OVERLAP = 320;        // ~200 Token\n\nconst content = $json.content || '';\nconst fileName = $json.fileName;\nconst sourceType = $json.sourceType;\nconst fileHash = $json.fileHash;\nconst modifiedAt = $json.modifiedAt;\n\nconst crypto = require('crypto');\n\nif (!content || content.length < 10) {\n  return [{ json: { error: 'No content to chunk', fileName } }];\n}\n\nconst chunks = [];\nlet startIndex = 0;\nlet chunkIndex = 0;\n\nwhile (startIndex < content.length) {\n  let endIndex = Math.min(startIndex + CHUNK_SIZE, content.length);\n  \n  // Versuche am Satzende zu brechen\n  if (endIndex < content.length) {\n    const lastPeriod = content.lastIndexOf('.', endIndex);\n    const lastNewline = content.lastIndexOf('\\n', endIndex);\n    const breakPoint = Math.max(lastPeriod, lastNewline);\n    if (breakPoint > startIndex + CHUNK_SIZE * 0.5) {\n      endIndex = breakPoint + 1;\n    }\n  }\n  \n  const chunkText = content.slice(startIndex, endIndex).trim();\n  \n  if (chunkText.length > 20) {\n    const chunkHash = crypto.createHash('sha256').update(chunkText).digest('hex');\n    chunks.push({\n      json: {\n        chunk_content: chunkText,\n        chunk_hash: chunkHash,\n        chunk_index: chunkIndex,\n        source_file: fileName,\n        source_type: sourceType,\n        file_hash: fileHash,\n        modified_at: modifiedAt\n      }\n    });\n    chunkIndex++;\n  }\n  \n  if (endIndex >= content.length) break;\n  startIndex = endIndex - OVERLAP;\n}\n\nconst totalChunks = chunks.length;\nchunks.forEach(c => c.json.total_chunks = totalChunks);\n\nreturn chunks;"
      },
      "id": "eddy-ingest-chunk",
      "name": "ğŸ”ª Chunking (800/320)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1440, 400]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://n8n-ollama:11434/api/embeddings",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify({ model: 'mxbai-embed-large', prompt: 'search_document: ' + $json.chunk_content }) }}",
        "options": {
          "timeout": 60000
        }
      },
      "id": "eddy-ingest-embed",
      "name": "ğŸ§¬ Embedding (mxbai)",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [1660, 400]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO document_chunks (content, content_hash, embedding, source_file, source_type, file_hash, chunk_index, total_chunks, source_modified_at) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9) ON CONFLICT (content_hash) DO NOTHING RETURNING id;",
        "options": {
          "queryReplacement": "={{ [$node['ğŸ”ª Chunking (800/320)'].json.chunk_content, $node['ğŸ”ª Chunking (800/320)'].json.chunk_hash, '[' + $json.embedding.join(',') + ']', $node['ğŸ”ª Chunking (800/320)'].json.source_file, $node['ğŸ”ª Chunking (800/320)'].json.source_type, $node['ğŸ”ª Chunking (800/320)'].json.file_hash, $node['ğŸ”ª Chunking (800/320)'].json.chunk_index, $node['ğŸ”ª Chunking (800/320)'].json.total_chunks, $node['ğŸ”ª Chunking (800/320)'].json.modified_at] }}"
        }
      },
      "id": "eddy-ingest-store",
      "name": "ğŸ’¾ Store in pgvector",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [1880, 400],
      "credentials": {
        "postgres": {
          "id": "Jq2IeHXVMOnpk0fI",
          "name": "eddy-knowledge-postgres"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Datei von pending nach processed verschieben\nconst fs = require('fs');\nconst path = require('path');\n\nconst filePath = $('ğŸ“‹ Has Files?').first().json.filePath;\nconst fileName = $('ğŸ“‹ Has Files?').first().json.fileName;\nconst processedDir = '/eddy-inbox/processed';\n\ntry {\n  if (!fs.existsSync(processedDir)) {\n    fs.mkdirSync(processedDir, { recursive: true });\n  }\n  \n  const dest = path.join(processedDir, fileName);\n  fs.renameSync(filePath, dest);\n  \n  return [{\n    json: {\n      status: 'success',\n      fileName,\n      movedTo: dest,\n      chunks_created: $items().length\n    }\n  }];\n} catch (err) {\n  const failedDir = '/eddy-inbox/failed';\n  try {\n    if (!fs.existsSync(failedDir)) fs.mkdirSync(failedDir, { recursive: true });\n    fs.renameSync(filePath, path.join(failedDir, fileName));\n  } catch (e) { /* ignore */ }\n  \n  return [{ json: { status: 'error', error: err.message, fileName } }];\n}"
      },
      "id": "eddy-ingest-move",
      "name": "ğŸ“ Move to Processed",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [2100, 400]
    },
    {
      "parameters": {
        "jsCode": "// Webhook-Pfad: Content direkt chunken\nconst crypto = require('crypto');\n\nconst content = $json.body.content || '';\nconst metadata = $json.body.metadata || {};\nconst fileName = metadata.source_file || 'webhook-input';\nconst sourceType = metadata.source_type || 'txt';\nconst category = metadata.category || null;\nconst tags = metadata.tags || [];\nconst fileHash = crypto.createHash('sha256').update(content).digest('hex');\n\nconst CHUNK_SIZE = 800;\nconst OVERLAP = 320;\n\nif (!content || content.trim().length < 10) {\n  return [{ json: { error: 'No content provided' } }];\n}\n\nconst chunks = [];\nlet startIndex = 0;\nlet chunkIndex = 0;\nconst trimmed = content.trim();\n\nwhile (startIndex < trimmed.length) {\n  let endIndex = Math.min(startIndex + CHUNK_SIZE, trimmed.length);\n  \n  if (endIndex < trimmed.length) {\n    const lastPeriod = trimmed.lastIndexOf('.', endIndex);\n    const lastNewline = trimmed.lastIndexOf('\\n', endIndex);\n    const breakPoint = Math.max(lastPeriod, lastNewline);\n    if (breakPoint > startIndex + CHUNK_SIZE * 0.5) {\n      endIndex = breakPoint + 1;\n    }\n  }\n  \n  const chunkText = trimmed.slice(startIndex, endIndex).trim();\n  \n  if (chunkText.length > 20) {\n    const chunkHash = crypto.createHash('sha256').update(chunkText).digest('hex');\n    chunks.push({\n      json: {\n        chunk_content: chunkText,\n        chunk_hash: chunkHash,\n        chunk_index: chunkIndex,\n        source_file: fileName,\n        source_type: sourceType,\n        file_hash: fileHash,\n        category: category,\n        tags: tags\n      }\n    });\n    chunkIndex++;\n  }\n  \n  if (endIndex >= trimmed.length) break;\n  startIndex = endIndex - OVERLAP;\n}\n\nconst totalChunks = chunks.length;\nchunks.forEach(c => c.json.total_chunks = totalChunks);\n\nreturn chunks;"
      },
      "id": "eddy-ingest-webhook-chunk",
      "name": "ğŸ”ª Webhook Chunking",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [340, 700]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "http://n8n-ollama:11434/api/embeddings",
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify({ model: 'mxbai-embed-large', prompt: 'search_document: ' + $json.chunk_content }) }}",
        "options": {
          "timeout": 60000
        }
      },
      "id": "eddy-ingest-webhook-embed",
      "name": "ğŸ§¬ WH Embedding",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [560, 700]
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO document_chunks (content, content_hash, embedding, source_file, source_type, file_hash, chunk_index, total_chunks, category, tags) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10) ON CONFLICT (content_hash) DO NOTHING RETURNING id;",
        "options": {
          "queryReplacement": "={{ [$node['ğŸ”ª Webhook Chunking'].json.chunk_content, $node['ğŸ”ª Webhook Chunking'].json.chunk_hash, '[' + $json.embedding.join(',') + ']', $node['ğŸ”ª Webhook Chunking'].json.source_file, $node['ğŸ”ª Webhook Chunking'].json.source_type, $node['ğŸ”ª Webhook Chunking'].json.file_hash, $node['ğŸ”ª Webhook Chunking'].json.chunk_index, $node['ğŸ”ª Webhook Chunking'].json.total_chunks, $node['ğŸ”ª Webhook Chunking'].json.category || null, $node['ğŸ”ª Webhook Chunking'].json.tags ? '{' + $node['ğŸ”ª Webhook Chunking'].json.tags.join(',') + '}' : null] }}"
        }
      },
      "id": "eddy-ingest-webhook-store",
      "name": "ğŸ’¾ WH Store",
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.5,
      "position": [780, 700],
      "credentials": {
        "postgres": {
          "id": "Jq2IeHXVMOnpk0fI",
          "name": "eddy-knowledge-postgres"
        }
      }
    },
    {
      "parameters": {
        "respondWith": "json",
        "responseBody": "={{ { status: 'success', chunks_ingested: $items().length, source: $node['ğŸ”ª Webhook Chunking'].first().json.source_file } }}",
        "options": {}
      },
      "id": "eddy-ingest-webhook-respond",
      "name": "âœ… WH Response",
      "type": "n8n-nodes-base.respondToWebhook",
      "typeVersion": 1.1,
      "position": [1000, 700]
    }
  ],
  "connections": {
    "â° Scan Schedule (15min)": {
      "main": [[{"node": "ğŸ“‚ Scan Inbox", "type": "main", "index": 0}]]
    },
    "ğŸ“‚ Scan Inbox": {
      "main": [[{"node": "ğŸ“‹ Has Files?", "type": "main", "index": 0}]]
    },
    "ğŸ“‹ Has Files?": {
      "main": [[{"node": "ğŸ” Duplikat-Check", "type": "main", "index": 0}]]
    },
    "ğŸ” Duplikat-Check": {
      "main": [[{"node": "ğŸ†• Noch nicht ingestiert?", "type": "main", "index": 0}]]
    },
    "ğŸ†• Noch nicht ingestiert?": {
      "main": [[{"node": "ğŸ“„ Text Extraction", "type": "main", "index": 0}]]
    },
    "ğŸ“„ Text Extraction": {
      "main": [[{"node": "ğŸ”ª Chunking (800/320)", "type": "main", "index": 0}]]
    },
    "ğŸ”ª Chunking (800/320)": {
      "main": [[{"node": "ğŸ§¬ Embedding (mxbai)", "type": "main", "index": 0}]]
    },
    "ğŸ§¬ Embedding (mxbai)": {
      "main": [[{"node": "ğŸ’¾ Store in pgvector", "type": "main", "index": 0}]]
    },
    "ğŸ’¾ Store in pgvector": {
      "main": [[{"node": "ğŸ“ Move to Processed", "type": "main", "index": 0}]]
    },
    "ğŸŒ Webhook Ingest": {
      "main": [[{"node": "ğŸ”ª Webhook Chunking", "type": "main", "index": 0}]]
    },
    "ğŸ”ª Webhook Chunking": {
      "main": [[{"node": "ğŸ§¬ WH Embedding", "type": "main", "index": 0}]]
    },
    "ğŸ§¬ WH Embedding": {
      "main": [[{"node": "ğŸ’¾ WH Store", "type": "main", "index": 0}]]
    },
    "ğŸ’¾ WH Store": {
      "main": [[{"node": "âœ… WH Response", "type": "main", "index": 0}]]
    }
  },
  "settings": {
    "executionOrder": "v1",
    "callerPolicy": "workflowsFromSameOwner",
    "availableInMCP": false
  },
  "pinData": {},
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "eddy-doc-ingest"
  }
}
